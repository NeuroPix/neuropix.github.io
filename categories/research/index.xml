<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research on NeuroPix</title>
    <link>https://neuropix.github.io/categories/research/</link>
    <description>Recent content in Research on NeuroPix</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://neuropix.github.io/categories/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CLIP：跨模态学习的突破</title>
      <link>https://neuropix.github.io/posts/clip%E8%B7%A8%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AA%81%E7%A0%B4/</link>
      <pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://neuropix.github.io/posts/clip%E8%B7%A8%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AA%81%E7%A0%B4/</guid>
      <description>&lt;p&gt;在深度学习的快速发展中，跨模态学习正逐渐成为一个重要的研究领域。OpenAI 提出的 CLIP（Contrastive Language–Image Pre-training）模型正是这一领域中的一项重要成果。CLIP 的目标是将文本和图像结合在一起，使得计算机能够理解图像与其描述之间的关系。&lt;/p&gt;&#xA;&lt;h2 id=&#34;什么是-clip&#34;&gt;什么是 CLIP？&lt;/h2&gt;&#xA;&lt;p&gt;CLIP 是一种预训练模型，它通过对大量的图像和相应的文本描述进行对比学习，从而学习到图像和文本之间的相互关系。与传统的视觉模型不同，CLIP 不仅关注图像本身，还将图像与相关文本信息结合起来。这种方法使得 CLIP 在多种视觉任务上展现出卓越的性能。&lt;/p&gt;&#xA;&lt;h2 id=&#34;clip-的工作原理&#34;&gt;CLIP 的工作原理&lt;/h2&gt;&#xA;&lt;p&gt;CLIP 的工作流程可以概括为以下几个步骤：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;数据收集&lt;/strong&gt;：CLIP 使用大量的图像和相应的文本描述进行训练。这些数据来自互联网，包括各种类型的图像和其描述。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;编码&lt;/strong&gt;：模型分别使用两个不同的编码器处理图像和文本。图像通过视觉编码器（通常是 CNN 或 Transformer）进行处理，而文本则通过文本编码器（通常是 Transformer）进行处理。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;对比学习&lt;/strong&gt;：CLIP 通过对比学习的方法，将图像和其描述的嵌入向量映射到同一个空间。在训练过程中，模型会努力最大化图像和文本之间的相似性，同时最小化不相关图像和文本之间的相似性。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;多任务能力&lt;/strong&gt;：经过训练后，CLIP 模型可以处理多种视觉任务，如图像分类、对象检测、图像生成等，而无需针对特定任务进行重新训练。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;clip-的应用&#34;&gt;CLIP 的应用&lt;/h2&gt;&#xA;&lt;p&gt;CLIP 的多任务学习能力使其在多个领域取得了显著的成功，以下是一些主要应用：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;图像分类&lt;/strong&gt;：CLIP 可以根据给定的文本标签进行图像分类，具有极高的灵活性。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;文本到图像检索&lt;/strong&gt;：用户可以输入文本描述，CLIP 会从大量图像中检索出与描述最相关的图像。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;图像生成&lt;/strong&gt;：CLIP 可以与生成模型结合，生成符合文本描述的图像。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;零样本学习&lt;/strong&gt;：由于 CLIP 在训练过程中学习了大量的图像和文本对，它能够在没有明确标签的情况下进行图像分类任务。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;&#xA;&lt;p&gt;CLIP 作为一种跨模态学习模型，成功地将图像和文本结合在一起，为计算机视觉和自然语言处理领域带来了新的可能性。它的多任务能力和灵活性使其成为一个重要的研究工具，推动了相关领域的进一步发展。随着对 CLIP 模型的深入研究，我们可以期待未来将出现更多的应用和改进。&lt;/p&gt;&#xA;&lt;p&gt;希望这篇文章能够帮助你了解 CLIP 的基本概念及其应用。如果你对这个话题有任何问题或想法，欢迎在评论区留言与我交流！&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformer 架构的演变与应用</title>
      <link>https://neuropix.github.io/posts/transformer-%E6%9E%B6%E6%9E%84%E7%9A%84%E6%BC%94%E5%8F%98%E4%B8%8E%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://neuropix.github.io/posts/transformer-%E6%9E%B6%E6%9E%84%E7%9A%84%E6%BC%94%E5%8F%98%E4%B8%8E%E5%BA%94%E7%94%A8/</guid>
      <description>&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;&#xA;&lt;p&gt;在深度学习的历史长河中，Transformer 架构的出现无疑是一个重要的里程碑。自 2017 年由 Vaswani 等人在论文《Attention is All You Need》中提出以来，Transformer 迅速在自然语言处理（NLP）领域崭露头角，并逐渐扩展到计算机视觉、语音识别等多个领域。本文将探讨 Transformer 的演变过程、其在不同应用中的重要性以及未来的发展趋势。&lt;/p&gt;&#xA;&lt;h2 id=&#34;transformer-的基本概念&#34;&gt;Transformer 的基本概念&lt;/h2&gt;&#xA;&lt;h3 id=&#34;自注意力机制&#34;&gt;自注意力机制&lt;/h3&gt;&#xA;&lt;p&gt;自注意力机制是 Transformer 的核心创新之一。它允许模型在处理输入序列时，动态地关注序列中的不同部分，从而捕捉长距离依赖关系。这一机制使得模型能够更加灵活地理解上下文信息。&lt;/p&gt;&#xA;&lt;h3 id=&#34;transformer-架构&#34;&gt;Transformer 架构&lt;/h3&gt;&#xA;&lt;p&gt;Transformer 由编码器（Encoder）和解码器（Decoder）组成：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;编码器&lt;/strong&gt;：将输入序列编码为上下文表示。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;解码器&lt;/strong&gt;：根据编码器的输出生成目标序列。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://neuropix.github.io/images/transformer-evolution-and-application/transformer-architecture.png&#34;&#xA;    alt=&#34;Transformer 架构示意图&#34; height=&#34;400&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Transformer 架构示意图&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;h2 id=&#34;transformer-的演变&#34;&gt;Transformer 的演变&lt;/h2&gt;&#xA;&lt;h3 id=&#34;早期模型bert-和-gpt&#34;&gt;早期模型：BERT 和 GPT&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;BERT (Bidirectional Encoder Representations from Transformers)&lt;/strong&gt;：引入了双向编码，利用 Masked Language Model 进行预训练，广泛应用于文本分类和问答系统。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;GPT (Generative Pre-trained Transformer)&lt;/strong&gt;：侧重于生成任务，采用单向自回归的方式，适用于文本生成和对话系统。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;最近的发展vision-transformer-vit&#34;&gt;最近的发展：Vision Transformer (ViT)&lt;/h3&gt;&#xA;&lt;p&gt;Vision Transformer 将 Transformer 架构引入计算机视觉领域，通过将图像划分为固定大小的块，并将其视为序列输入，显著提升了图像分类性能。ViT 的成功表明 Transformer 也可以有效处理视觉数据。&lt;/p&gt;&#xA;&lt;h3 id=&#34;其他变种&#34;&gt;其他变种&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;T5 (Text-to-Text Transfer Transformer)&lt;/strong&gt;：将所有 NLP 任务统一为文本到文本的格式，提升了任务间的迁移学习能力。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Swin Transformer&lt;/strong&gt;：提出了一种分层结构，能够处理不同分辨率的输入，适用于图像分割和目标检测。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;transformer-的应用&#34;&gt;Transformer 的应用&lt;/h2&gt;&#xA;&lt;h3 id=&#34;自然语言处理&#34;&gt;自然语言处理&lt;/h3&gt;&#xA;&lt;p&gt;Transformer 已成为 NLP 领域的标准架构，广泛应用于文本生成、机器翻译和情感分析等任务。&lt;/p&gt;</description>
    </item>
    <item>
      <title>视觉 Transformer（ViT）的介绍与应用</title>
      <link>https://neuropix.github.io/posts/%E8%A7%86%E8%A7%89-transformervit%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://neuropix.github.io/posts/%E8%A7%86%E8%A7%89-transformervit%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%BA%94%E7%94%A8/</guid>
      <description>&lt;p&gt;在过去的几年中，深度学习领域取得了巨大的进展，尤其是在计算机视觉任务中。传统上，卷积神经网络（CNN）是处理图像的主要工具。然而，近年来，视觉 Transformer（ViT）作为一种新的架构，逐渐受到研究人员的关注。&lt;/p&gt;&#xA;&lt;h2 id=&#34;什么是视觉-transformervit&#34;&gt;什么是视觉 Transformer（ViT）？&lt;/h2&gt;&#xA;&lt;p&gt;视觉 Transformer（ViT）是由 Google Research 提出的，它将 Transformer 架构应用于计算机视觉领域。ViT 通过将图像划分为若干个小的 patch，并将这些 patch 线性嵌入到 Transformer 中进行处理，从而实现图像分类等任务。该方法的核心思想是利用 Transformer 的自注意力机制来捕捉图像中的长距离依赖关系。&lt;/p&gt;&#xA;&lt;h2 id=&#34;vit-的主要特点&#34;&gt;ViT 的主要特点&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Patch 分割&lt;/strong&gt;：ViT 将输入图像划分为固定大小的 patch（例如 16x16），然后将每个 patch 展平并嵌入到一个高维空间中。这一过程将图像转换为一系列向量，为 Transformer 的输入做好准备。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;位置编码&lt;/strong&gt;：由于 Transformer 不具有 CNN 的空间结构感知能力，ViT 引入了位置编码，以便在处理图像时保留每个 patch 的位置信息。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;自注意力机制&lt;/strong&gt;：ViT 利用 Transformer 的自注意力机制，使得模型能够关注图像中的不同区域，从而捕捉到图像中更复杂的特征。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;预训练与微调&lt;/strong&gt;：ViT 通常在大规模数据集上进行预训练，然后在特定任务上进行微调。这种方法在多个计算机视觉基准上取得了显著的性能提升。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;vit-的应用&#34;&gt;ViT 的应用&lt;/h2&gt;&#xA;&lt;p&gt;视觉 Transformer 在许多计算机视觉任务中显示出了优异的性能，以下是一些具体应用：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;图像分类&lt;/strong&gt;：ViT 在大规模图像分类任务上表现出色，如 ImageNet 挑战。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;目标检测&lt;/strong&gt;：将 ViT 与目标检测框架结合，可以有效提高目标检测的准确性。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;图像分割&lt;/strong&gt;：ViT 也被应用于语义分割任务，通过自注意力机制实现更精细的分割效果。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;生成模型&lt;/strong&gt;：ViT 可以用于生成任务，如图像生成和图像超分辨率。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;&#xA;&lt;p&gt;视觉 Transformer（ViT）作为一种新的架构，正在逐渐改变计算机视觉的研究和应用。它的自注意力机制和高效的特征提取能力使得 ViT 成为解决许多视觉任务的有效工具。随着对 Transformer 架构的深入研究，我们可以期待未来会有更多的创新和改进出现。&lt;/p&gt;&#xA;&lt;p&gt;希望这篇文章能够帮助你理解视觉 Transformer 的基本概念及其应用。如果你有任何问题或想法，请在评论区留言与我交流！&lt;/p&gt;</description>
    </item>
    <item>
      <title>动态规划：背包问题</title>
      <link>https://neuropix.github.io/posts/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 08 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://neuropix.github.io/posts/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</guid>
      <description>&lt;p&gt;今天来简单聊聊动态规划中典型的背包问题。&lt;/p&gt;&#xA;&lt;h2 id=&#34;01-背包问题&#34;&gt;01 背包问题&lt;/h2&gt;&#xA;&lt;p&gt;先来看看下面这样一道题目吧。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Problem Description:&lt;/strong&gt; Many years ago, in Teddy’s hometown there was a man who was called “Bone Collector”. This man liked to collect various bones, such as dog’s, cow’s, and he also went to the grave. The bone collector had a big bag with a volume of V, and along his trip of collecting, there are a lot of bones. Obviously, different bones have different values and different volumes. Now given each bone’s value along his trip, can you calculate out the maximum total value the bone collector can get?&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
