<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research on NeuroPix</title>
    <link>https://neuropix.github.io/categories/research/</link>
    <description>Recent content in Research on NeuroPix</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://neuropix.github.io/categories/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CLIP：跨模态学习的突破</title>
      <link>https://neuropix.github.io/posts/clip%E8%B7%A8%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AA%81%E7%A0%B4/</link>
      <pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://neuropix.github.io/posts/clip%E8%B7%A8%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AA%81%E7%A0%B4/</guid>
      <description>CLIP（Contrastive Language–Image Pre-training）是由 OpenAI 提出的跨模态学习模型，旨在通过对比学习将图像与其文本描述结合起来，从而使计算机能够理解两者之间的关系。该模型通过数据收集、编码、对比学习等步骤进行训练，展现出多任务能力，能够处理图像分类、文本到图像检索、图像生成等多种视觉任务，且无需针对特定任务重新训练。CLIP 的灵活性和多任务能力为计算机视觉和自然语言处理领域带来了新的可能性，推动了相关研究的发展。</description>
    </item>
    <item>
      <title>Transformer 架构的演变与应用</title>
      <link>https://neuropix.github.io/posts/transformer-%E6%9E%B6%E6%9E%84%E7%9A%84%E6%BC%94%E5%8F%98%E4%B8%8E%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://neuropix.github.io/posts/transformer-%E6%9E%B6%E6%9E%84%E7%9A%84%E6%BC%94%E5%8F%98%E4%B8%8E%E5%BA%94%E7%94%A8/</guid>
      <description>本文探讨了Transformer架构的演变及其在多个领域的重要性。自2017年Vaswani等人提出Transformer以来，自注意力机制成为其核心创新，使模型能灵活捕捉长距离依赖。早期模型如BERT和GPT引领了自然语言处理的发展，Vision Transformer（ViT）将该架构引入计算机视觉领域并显著提升了图像分类性能。Transformer在自然语言处理、计算机视觉和语音识别等应用中表现出色，未来预计将在多模态学习和实时处理等领域继续创新和发展。</description>
    </item>
    <item>
      <title>视觉 Transformer（ViT）的介绍与应用</title>
      <link>https://neuropix.github.io/posts/%E8%A7%86%E8%A7%89-transformervit%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%BA%94%E7%94%A8/</link>
      <pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://neuropix.github.io/posts/%E8%A7%86%E8%A7%89-transformervit%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%BA%94%E7%94%A8/</guid>
      <description>视觉 Transformer（ViT）是 Google Research 提出的新型计算机视觉架构，它通过将图像分割成小的 patch 并利用 Transformer 的自注意力机制来处理图像，捕捉长距离依赖关系。ViT 的主要特点包括 patch 分割、位置编码和在大规模数据集上的预训练与微调，使其在图像分类、目标检测、图像分割和生成任务等多个计算机视觉领域表现出色。ViT 正在逐渐改变计算机视觉的研究和应用，为未来的创新和改进奠定基础。</description>
    </item>
    <item>
      <title>动态规划：背包问题</title>
      <link>https://neuropix.github.io/posts/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 08 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://neuropix.github.io/posts/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</guid>
      <description>背包问题是一种经典的动态规划问题，旨在在给定容量的情况下最大化物品的总价值。以“骨头收藏家”为例，每块骨头具有特定的体积和价值，骨头收藏家需要选择哪些骨头以获得最大的总价值。通过定义递推关系和状态转移方程，我们可以使用递归或动态规划数组来解决这个问题。分为“01背包问题”和“完全背包问题”，前者限制每种物品的数量，后者允许任意数量的物品选择。通过有效地管理和优化状态转移，能够在合理的时间复杂度内计算出最优解。</description>
    </item>
  </channel>
</rss>
